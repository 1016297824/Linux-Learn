// Hadoop安装部署

1.Hadoop安装：
(1)下载
    wget http://archive.apache.org/dist/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz 
(2)解压
    tar -zxvf hadoop-3.3.0.tar.gz -C /export/server/
(3)创建软链接
    ln -s /export/server/hadoop-3.3.0 /export/server/hadoop

2.修改Hadoop配置文件1：hadoop-env.sh
(1)打开配置文件    
    vim /export/server/hadoop/etc/hadoop/hadoop-env.sh
(2)编辑配置信息：在文件开头加入
    # 配置Java安装路径
    export JAVA_HOME=/export/server/jdk
    # 配置Hadoop安装路径
    export HADOOP_HOME=/export/server/hadoop
    # Hadoop hdfs配置文件路径
    export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
    # Hadoop YARN配置文件路径
    export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
    # Hadoop YARN 日志文件夹
    export YARN_LOG_DIR=$HADOOP_HOME/logs/yarn
    # Hadoop hdfs 日志文件夹
    export HADOOP_LOG_DIR=$HADOOP_HOME/logs/hdfs
    # Hadoop的使用启动用户配置
    export HDFS_NAMENODE_USER=root
    export HDFS_DATANODE_USER=root
    export HDFS_SECONDARYNAMENODE_USER=root
    export YARN_RESOURCEMANAGER_USER=root
    export YARN_NODEMANAGER_USER=root
    export YARN_PROXYSERVER_USER=root

3.修改Hadoop配置文件2：core-site.xml
(1)打开配置文件    
    vim /export/server/hadoop/etc/hadoop/core-site.xml
(2)编辑配置信息：清空配置，填入如下内容
    <configuration>
        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://node1:8020</value>
            <description></description>
        </property>
        <property>
            <name>io.file.buffer.size</name>
            <value>131072</value>
            <description></description>
        </property>
    </configuration>

4.修改Hadoop配置文件3：hdfs-site.xml
(1)打开配置文件    
    vim /export/server/hadoop/etc/hadoop/hdfs-site.xml
(2)编辑配置信息：清空配置，填入如下内容
    <configuration>
        <property>
            <name>dfs.datanode.data.dir.perm</name>
            <value>700</value>
        </property>
        <property>
            <name>dfs.namenode.name.dir</name>
            <value>/data/nn</value>
            <description>Path on the local filesystem where the NameNode stores the namespace and transactions logs persistently.</description>
        </property>
        <property>
            <name>dfs.namenode.hosts</name>
            <value>node1,node2,node3</value>
            <description>List of permitted DataNodes.</description>
        </property>
        <property>
            <name>dfs.blocksize</name>
            <value>268435456</value>
            <description></description>
        </property>
        <property>
            <name>dfs.namenode.handler.count</name>
            <value>100</value>
            <description></description>
        </property>
        <property>
            <name>dfs.datanode.data.dir</name>
            <value>/data/dn</value>
        </property>
    </configuration>

5.修改Hadoop配置文件4：mapred-env.sh
(1)打开配置文件    
    vim /export/server/hadoop/etc/hadoop/mapred-env.sh
(2)编辑配置信息：在文件的开头加入如下环境变量设置
    export JAVA_HOME=/export/server/jdk
    export HADOOP_JOB_HISTORYSERVER_HEAPSIZE=1000
    export HADOOP_MAPRED_ROOT_LOGGER=INFO,RFA

6.修改Hadoop配置文件5：mapred-site.xml
(1)打开配置文件    
    vim /export/server/hadoop/etc/hadoop/mapred-site.xml
(2)编辑配置信息：清空配置，填入如下内容
    <configuration>
        <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
            <description></description>
        </property>
        <property>
            <name>mapreduce.jobhistory.address</name>
            <value>node1:10020</value>
            <description></description>
        </property>
        <property>
            <name>mapreduce.jobhistory.webapp.address</name>
            <value>node1:19888</value>
            <description></description>
        </property>
        <property>
            <name>mapreduce.jobhistory.intermediate-done-dir</name>
            <value>/data/mr-history/tmp</value>
            <description></description>
        </property>
        <property>
            <name>mapreduce.jobhistory.done-dir</name>
            <value>/data/mr-history/done</value>
            <description></description>
        </property>
        <property>
            <name>yarn.app.mapreduce.am.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
        </property>
        <property>
            <name>mapreduce.map.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
        </property>
        <property>
            <name>mapreduce.reduce.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
        </property>
    </configuration>

7.修改Hadoop配置文件6：yarn-env.sh
(1)打开配置文件    
    vim /export/server/hadoop/etc/hadoop/yarn-env.sh
(2)编辑配置信息：在文件开头，填入如下内容
    export JAVA_HOME=/export/server/jdk
    export HADOOP_HOME=/export/server/hadoop
    export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
    export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
    export YARN_LOG_DIR=$HADOOP_HOME/logs/yarn
    export HADOOP_LOG_DIR=$HADOOP_HOME/logs/hdfs

8.修改Hadoop配置文件7：yarn-site.xml
(1)打开配置文件    
    vim /export/server/hadoop/etc/hadoop/yarn-site.xml
(2)编辑配置信息：在文件开头，填入如下内容
    <configuration>
        <property>
            <name>yarn.nodemanager.address</name>
            <value>0.0.0.0:8041</value>
        </property>
        <property>
            <name>yarn.log.server.url</name>
            <value>http://node1:19888/jobhistory/logs</value>
            <description></description>
        </property>
        <property>
            <name>yarn.web-proxy.address</name>
            <value>node1:8089</value>
            <description>proxy server hostname and port</description>
        </property>
        <property>
            <name>yarn.log-aggregation-enable</name>
            <value>true</value>
            <description>Configuration to enable or disable log aggregation</description>
        </property>
        <property>
            <name>yarn.nodemanager.remote-app-log-dir</name>
            <value>/tmp/logs</value>
            <description>Configuration to enable or disable log aggregation</description>
        </property>
        <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>node1</value>
            <description></description>
        </property>
        <property>
            <name>yarn.resourcemanager.scheduler.class</name>
            <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
            <description></description>
        </property>
        <property>
            <name>yarn.nodemanager.local-dirs</name>
            <value>/data/nm-local</value>
            <description>Comma-separated list of paths on the local filesystem where intermediate data is written.</description>
        </property>
        <property>
            <name>yarn.nodemanager.log-dirs</name>
            <value>/data/nm-log</value>
            <description>Comma-separated list of paths on the local filesystem where logs are written.</description>
        </property>
        <property>
            <name>yarn.nodemanager.log.retain-seconds</name>
            <value>10800</value>
            <description>Default time (in seconds) to retain log files on the NodeManager Only applicable if log-aggregation is disabled.</description>
        </property>
        <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
            <description>Shuffle service that needs to be set for Map Reduce applications.</description>
        </property>
    </configuration>

9.修改Hadoop配置文件8：workers
(1)打开配置文件    
    vim /export/server/hadoop/etc/hadoop/workers
(2)编辑配置信息：全部内容如下
    node1
    node2
    node3

10.将Hadoop复制到其它机器，并创建软链接
    cd /export/server
    scp -r hadoop-3.3.0 node2:`pwd`/
    scp -r hadoop-3.3.0 node3:`pwd`/

11.在node1执行
    mkdir -p /data/nn
    mkdir -p /data/dn
    mkdir -p /data/nm-log
    mkdir -p /data/nm-local

12.在node2，node3执行
    mkdir -p /data/dn
    mkdir -p /data/nm-log
    mkdir -p /data/nm-local

13.修改node1,node2,node3环境变量
(1)打开环境变量文件
    vim /etc/profile
(2)加入一下内容
    export HADOOP_HOME=/export/server/hadoop
    export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
(3)刷新生效
    source /etc/profile

14.启动：由于SSH免密登录，node1会启动node2,node3的Hadoop
(1)格式化Namenode，在node1执行
    hadoop namenode -format
(2)启动hadoop的hdfs集群，在node1执行
    start-dfs.sh
(3)启动hadoop的yarn集群，在node1执行
    start-yarn.sh
(4)启动历史服务器，在node1执行
    mapred --daemon start historyserver
(5)启动web代理服务器，在node1执行
    yarn-daemon.sh start proxyserver

15.验证Hadoop运行情况
(1)在node1、node2、node3上通过jps验证进程是否都启动成功
    1)在node1中可以看到如下进程
        NameNode
        DataNode
        NodeManager
        ResourceManager
        WebAppProxyServer
        JobHistoryServer
    2)在node2,node3中可以看到如下进程
        DataNode
        NodeManager
(2)验证HDFS，浏览器打开：http: node1:9870
    1)创建文件test.txt，随意填入内容，并执行：
        hadoop fs -put test.txt /test
        hadoop fs -cat /test
(3)验证YARN，浏览器打开：http: node1:8088
    1)创建文件words.txt，填入如下内容
        zhangkun itcast hadoop
        zhangkun hadoop hadoop
        zhangkun itcast
    2)将文件上传到HDFS中
        hadoop fs -put words.txt /words
    3)执行如下命令验证YARN是否正常
        hadoop jar /export/server/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar wordcount -Dmapred.job.queue.name=root.root /words /output

16.开放端口
    firewall-cmd --add-port=[端口号]/tcp --permanent
    firewall-cmd --reload
(1)node1端口详解：9870,8088,8020,8030
    1)9870:HDFS查看端口
    2)8088:YARN查看端口
    3)8020:此端为NameNode(RPC)端口，在core-site.xml配置文件中，每台服务器要设置相同（默认为8020）
    4)8030:此端为ResourceManager(RPC)端口，在yarn-site.xml配置文件中，每台服务器要设置相同（默认为8030）
(2)node2,node3端口详解：9866,8041
    1)9866:DataNode的默认数据传输端口
    2)8041:自定义端口，该配置项定义了 NodeManager 服务监听的地址和端口

17.创建包装脚本
(1)创建一个新的启动脚本：start-hadoop-customer.sh
    #!/bin/bash

    # 获取Hadoop主目录
    HADOOP_HOME=$(dirname $(dirname $(readlink -f $0)))

    # 启动HDFS
    echo "Starting HDFS..."
    $HADOOP_HOME/sbin/start-dfs.sh

    # 启动YARN
    echo "Starting YARN..."
    $HADOOP_HOME/sbin/start-yarn.sh

    # 启动History Server
    echo "Starting MapReduce History Server..."
    $HADOOP_HOME/bin/mapred --daemon start historyserver

    if [ $? -eq 0 ]; then
      echo "MapReduce History Server started successfully"
    else
      echo "Failed to start MapReduce History Server"
    fi

    echo "Hadoop cluster with History Server started"
(2)创建对应的停止脚本：stop-hadoop-customer.sh
    #!/bin/bash

    # 获取Hadoop主目录
    HADOOP_HOME=$(dirname $(dirname $(readlink -f $0)))

    # 停止History Server
    echo "Stopping MapReduce History Server..."
    $HADOOP_HOME/bin/mapred --daemon stop historyserver

    # 停止YARN
    echo "Stopping YARN..."
    $HADOOP_HOME/sbin/stop-yarn.sh

    # 停止HDFS
    echo "Stopping HDFS..."
    $HADOOP_HOME/sbin/stop-dfs.sh

    echo "Hadoop cluster with History Server stopped"
(3)给脚本添加执行权限
    chmod +x start-hadoop-customer.sh
    chmod +x stop-hadoop-customer.sh

18.设置开机自启
(1)创建hadoop用户并添加权限
    useradd hadoop
    passwd hadoop
    chown -R hadoop:hadoop /export/server/hadoop
    chown -R hadoop:hadoop /export/server/hadoop-3.3.0/
    chown -R hadoop:hadoop /data
    注意：更改hadoop-env.sh中的用户启动权限
(2)主节点设置：/etc/systemd/system/hadoop-master.service
    [Unit]
    Description=Hadoop Master Services
    After=network.target sshd.service
    Wants=network.target

    [Service]
    Type=forking
    User=hadoop
    Group=hadoop
    Environment="JAVA_HOME=/export/server/jdk"
    Environment="HADOOP_HOME=/export/server/hadoop"
    Environment="HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop"
    ExecStart=/export/server/hadoop/sbin/start-hadoop-customer.sh
    ExecStop=/export/server/hadoop/sbin/stop-hadoop-customer.sh
    RemainAfterExit=yes
    TimeoutSec=300
    GuessMainPID=no

    [Install]
    WantedBy=multi-user.target
(3)主节点启动服务
    systemctl daemon-reload
    systemctl start hadoop-master.service
    systemctl enable hadoop-master.service
(4)从节点设置：/etc/systemd/system/hadoop-slave.service
    [Unit]
    Description=Hadoop Slave Services
    After=network.target sshd.service
    Wants=network.target

    [Service]
    Type=forking
    User=hadoop
    Group=hadoop
    Environment="JAVA_HOME=/export/server/jdk"
    Environment="HADOOP_HOME=/export/server/hadoop"
    Environment="HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop"
    # 只启动从节点应该运行的服务
    ExecStartPre=/bin/bash -c 'echo "HADOOP_HOME=$HADOOP_HOME"'
    ExecStart=/bin/bash -c "${HADOOP_HOME}/bin/hdfs --daemon start datanode && ${HADOOP_HOME}/bin/yarn --daemon start nodemanager"
    ExecStop=/bin/bash -c "${HADOOP_HOME}/bin/yarn --daemon stop nodemanager && ${HADOOP_HOME}/bin/hdfs --daemon stop datanode"
    RemainAfterExit=yes
    TimeoutSec=300

    [Install]
    WantedBy=multi-user.target
(5)从节点启动服务
    systemctl daemon-reload
    systemctl start hadoop-slave.service
    systemctl enable hadoop-slave.service